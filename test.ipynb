{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 , Loss:54381120.0000\n",
      "Epoch:1 , Loss:134873408.0000\n",
      "Epoch:2 , Loss:540817984.0000\n",
      "Epoch:3 , Loss:852627072.0000\n",
      "Epoch:4 , Loss:19939966.0000\n",
      "Epoch:5 , Loss:11648721.0000\n",
      "Epoch:6 , Loss:7670533.5000\n",
      "Epoch:7 , Loss:5418386.5000\n",
      "Epoch:8 , Loss:4022973.5000\n",
      "Epoch:9 , Loss:3106779.5000\n",
      "Epoch:10 , Loss:2479914.2500\n",
      "Epoch:11 , Loss:2037791.6250\n",
      "Epoch:12 , Loss:1717498.5000\n",
      "Epoch:13 , Loss:1479760.8750\n",
      "Epoch:14 , Loss:1299571.5000\n",
      "Epoch:15 , Loss:1160008.1250\n",
      "Epoch:16 , Loss:1049513.0000\n",
      "Epoch:17 , Loss:960176.3750\n",
      "Epoch:18 , Loss:886646.6875\n",
      "Epoch:19 , Loss:824863.5625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n",
    "\n",
    "x = torch.randn(batch_n,input_data)\n",
    "y = torch.randn(batch_n,output_data)\n",
    " \n",
    "w1 = torch.randn(input_data,hidden_layer)\n",
    "w2 = torch.randn(hidden_layer,output_data)\n",
    "\n",
    "epoch_n = 20\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    h1 = x.mm(w1)  # 100*1000\n",
    "    h1 = h1.clamp(min=0)\n",
    "    y_pred = h1.mm(w2)  # 100*10\n",
    "    # print(y_pred)\n",
    " \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"Epoch:{} , Loss:{:.4f}\".format(epoch, loss))\n",
    " \n",
    "    gray_y_pred = 2 * (y_pred - y)\n",
    "    gray_w2 = h1.t().mm(gray_y_pred)\n",
    " \n",
    "    grad_h = gray_y_pred.clone()\n",
    "    grad_h = grad_h.mm(w2.t())\n",
    "    grad_h.clamp_(min=0)\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    " \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * gray_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 , Loss:70035344.0000\n",
      "Epoch:1 , Loss:222667856.0000\n",
      "Epoch:2 , Loss:794220096.0000\n",
      "Epoch:3 , Loss:440443872.0000\n",
      "Epoch:4 , Loss:10189192.0000\n",
      "Epoch:5 , Loss:7347922.5000\n",
      "Epoch:6 , Loss:5635929.0000\n",
      "Epoch:7 , Loss:4491406.5000\n",
      "Epoch:8 , Loss:3678770.0000\n",
      "Epoch:9 , Loss:3074904.2500\n",
      "Epoch:10 , Loss:2611605.0000\n",
      "Epoch:11 , Loss:2247637.7500\n",
      "Epoch:12 , Loss:1956895.5000\n",
      "Epoch:13 , Loss:1720250.3750\n",
      "Epoch:14 , Loss:1524806.6250\n",
      "Epoch:15 , Loss:1361603.7500\n",
      "Epoch:16 , Loss:1224100.0000\n",
      "Epoch:17 , Loss:1107220.2500\n",
      "Epoch:18 , Loss:1007194.6875\n",
      "Epoch:19 , Loss:920878.2500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# 批量输入的数据量\n",
    "batch_n = 100\n",
    "# 通过隐藏层后输出的特征数\n",
    "hidden_layer = 100\n",
    "# 输入数据的特征个数\n",
    "input_data = 1000\n",
    "# 最后输出的分类结果数\n",
    "output_data = 10\n",
    "\n",
    "x = Variable(torch.randn(batch_n , input_data) , requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n , output_data) , requires_grad = False)\n",
    " \n",
    "w1 = Variable(torch.randn(input_data,hidden_layer),requires_grad = True)\n",
    "w2 = Variable(torch.randn(hidden_layer,output_data),requires_grad = True)\n",
    "\n",
    "epoch_n = 20\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    " \n",
    "    y_pred = x.mm(w1).clamp(min= 0 ).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"Epoch:{} , Loss:{:.4f}\".format(epoch, loss.data))\n",
    " \n",
    "    loss.backward()\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    " \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# 批量输入的数据量\n",
    "batch_n = 100\n",
    "# 通过隐藏层后输出的特征数\n",
    "hidden_layer = 100\n",
    "# 输入数据的特征个数\n",
    "input_data = 1000\n",
    "# 最后输出的分类结果数\n",
    "output_data = 10\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "     \n",
    "    def forward(self,input,w1,w2):\n",
    "        x = torch.mm(input,w1)\n",
    "        x = torch.clamp(x,min=0)\n",
    "        x = torch.mm(x,w2)\n",
    "        return x\n",
    "     \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# 批量输入的数据量\n",
    "batch_n = 100\n",
    "# 通过隐藏层后输出的特征数\n",
    "hidden_layer = 100\n",
    "# 输入数据的特征个数\n",
    "input_data = 1000\n",
    "# 最后输出的分类结果数\n",
    "output_data = 10\n",
    " \n",
    "x = Variable(torch.randn(batch_n , input_data) , requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n , output_data) , requires_grad = False)\n",
    "\n",
    "models = torch.nn.Sequential(\n",
    "    # 首先通过其完成从输入层到隐藏层的线性变换\n",
    "    torch.nn.Linear(input_data,hidden_layer),\n",
    "    # 经过激活函数\n",
    "    torch.nn.ReLU(),\n",
    "    # 最后完成从隐藏层到输出层的线性变换\n",
    "    torch.nn.Linear(hidden_layer,output_data)\n",
    ")\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (Linel): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (ReLU1): ReLU()\n",
      "  (Line2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    " \n",
    "# 批量输入的数据量\n",
    "batch_n = 100\n",
    "# 通过隐藏层后输出的特征数\n",
    "hidden_layer = 100\n",
    "# 输入数据的特征个数\n",
    "input_data = 1000\n",
    "# 最后输出的分类结果数\n",
    "output_data = 10\n",
    " \n",
    "models = torch.nn.Sequential(OrderedDict([\n",
    "    (\"Linel\",torch.nn.Linear(input_data,hidden_layer)),\n",
    "    (\"ReLU1\",torch.nn.ReLU()),\n",
    "    (\"Line2\",torch.nn.Linear(hidden_layer,output_data))\n",
    "])\n",
    ")\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0,Loss:0.9964\n",
      "Epoch:1000,Loss:0.9180\n",
      "Epoch:2000,Loss:0.8520\n",
      "Epoch:3000,Loss:0.7950\n",
      "Epoch:4000,Loss:0.7452\n",
      "Epoch:5000,Loss:0.7009\n",
      "Epoch:6000,Loss:0.6609\n",
      "Epoch:7000,Loss:0.6239\n",
      "Epoch:8000,Loss:0.5894\n",
      "Epoch:9000,Loss:0.5572\n"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# 批量输入的数据量\n",
    "batch_n = 100\n",
    "# 通过隐藏层后输出的特征数\n",
    "hidden_layer = 100\n",
    "# 输入数据的特征个数\n",
    "input_data = 1000\n",
    "# 最后输出的分类结果数\n",
    "output_data = 10\n",
    " \n",
    "x = Variable(torch.randn(batch_n , input_data) , requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n , output_data) , requires_grad = False)\n",
    " \n",
    "models = torch.nn.Sequential(\n",
    "    # 首先通过其完成从输入层到隐藏层的线性变换\n",
    "    torch.nn.Linear(input_data,hidden_layer),\n",
    "    # 经过激活函数\n",
    "    torch.nn.ReLU(),\n",
    "    # 最后完成从隐藏层到输出层的线性变换\n",
    "    torch.nn.Linear(hidden_layer,output_data)\n",
    ")\n",
    "# print(models)\n",
    " \n",
    "epoch_n = 10000\n",
    "learning_rate = 1e-4\n",
    "loss_fn = torch.nn.MSELoss()\n",
    " \n",
    "for epoch in range(epoch_n):\n",
    "    y_pred = models(x)\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    if epoch%1000 == 0:\n",
    "        print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss.data))\n",
    "    models.zero_grad()\n",
    " \n",
    "    loss.backward()\n",
    " \n",
    "    for param in models.parameters():\n",
    "        param.data -= param.grad.data*learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:0.9800\n",
      "Epoch:1, Loss:0.9591\n",
      "Epoch:2, Loss:0.9388\n",
      "Epoch:3, Loss:0.9189\n",
      "Epoch:4, Loss:0.8995\n",
      "Epoch:5, Loss:0.8806\n",
      "Epoch:6, Loss:0.8622\n",
      "Epoch:7, Loss:0.8442\n",
      "Epoch:8, Loss:0.8267\n",
      "Epoch:9, Loss:0.8097\n",
      "Epoch:10, Loss:0.7930\n",
      "Epoch:11, Loss:0.7769\n",
      "Epoch:12, Loss:0.7612\n",
      "Epoch:13, Loss:0.7459\n",
      "Epoch:14, Loss:0.7309\n",
      "Epoch:15, Loss:0.7162\n",
      "Epoch:16, Loss:0.7019\n",
      "Epoch:17, Loss:0.6879\n",
      "Epoch:18, Loss:0.6741\n",
      "Epoch:19, Loss:0.6607\n",
      "Epoch:20, Loss:0.6476\n",
      "Epoch:21, Loss:0.6349\n",
      "Epoch:22, Loss:0.6225\n",
      "Epoch:23, Loss:0.6104\n",
      "Epoch:24, Loss:0.5985\n",
      "Epoch:25, Loss:0.5869\n",
      "Epoch:26, Loss:0.5755\n",
      "Epoch:27, Loss:0.5643\n",
      "Epoch:28, Loss:0.5534\n",
      "Epoch:29, Loss:0.5426\n",
      "Epoch:30, Loss:0.5321\n",
      "Epoch:31, Loss:0.5217\n",
      "Epoch:32, Loss:0.5115\n",
      "Epoch:33, Loss:0.5015\n",
      "Epoch:34, Loss:0.4918\n",
      "Epoch:35, Loss:0.4822\n",
      "Epoch:36, Loss:0.4728\n",
      "Epoch:37, Loss:0.4636\n",
      "Epoch:38, Loss:0.4546\n",
      "Epoch:39, Loss:0.4457\n"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# 批量输入的数据量\n",
    "batch_n = 100\n",
    "# 通过隐藏层后输出的特征数\n",
    "hidden_layer = 100\n",
    "# 输入数据的特征个数\n",
    "input_data = 1000\n",
    "# 最后输出的分类结果数\n",
    "output_data = 10\n",
    " \n",
    "x = Variable(torch.randn(batch_n , input_data) , requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n , output_data) , requires_grad = False)\n",
    " \n",
    "models = torch.nn.Sequential(\n",
    "    # 首先通过其完成从输入层到隐藏层的线性变换\n",
    "    torch.nn.Linear(input_data,hidden_layer),\n",
    "    # 经过激活函数\n",
    "    torch.nn.ReLU(),\n",
    "    # 最后完成从隐藏层到输出层的线性变换\n",
    "    torch.nn.Linear(hidden_layer,output_data)\n",
    ")\n",
    "# print(models)\n",
    " \n",
    "epoch_n = 40\n",
    "learning_rate = 1e-4\n",
    "loss_fn = torch.nn.MSELoss()\n",
    " \n",
    "optimzer = torch.optim.Adam(models.parameters(),lr = learning_rate)\n",
    " \n",
    "#进行模型训练\n",
    "for epoch in range(epoch_n):\n",
    "    y_pred = models(x)\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    print(\"Epoch:{}, Loss:{:.4f}\".format(epoch, loss.data))\n",
    "    optimzer.zero_grad()\n",
    " \n",
    "    loss.backward()\n",
    " \n",
    "    #进行梯度更新\n",
    "    optimzer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1), tensor(2), tensor(4), tensor(1), tensor(5), tensor(8), tensor(1), tensor(4), tensor(0), tensor(7), tensor(2), tensor(8), tensor(1), tensor(4), tensor(0), tensor(7), tensor(7), tensor(6), tensor(2), tensor(6), tensor(8), tensor(5), tensor(4), tensor(2), tensor(0), tensor(4), tensor(1), tensor(8), tensor(1), tensor(3), tensor(8), tensor(4), tensor(8), tensor(6), tensor(0), tensor(0), tensor(6), tensor(8), tensor(7), tensor(9), tensor(6), tensor(5), tensor(9), tensor(4), tensor(9), tensor(2), tensor(3), tensor(0), tensor(5), tensor(3), tensor(8), tensor(8), tensor(8), tensor(6), tensor(3), tensor(1), tensor(9), tensor(2), tensor(1), tensor(0), tensor(9), tensor(0), tensor(0), tensor(7)]\n",
      "Epoch  0/5\n",
      "----------\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 8000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9377cd5f35ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# 有GPU加下面这行，没有不用加\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# print(y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\kewill\\anaconda\\envs\\AI\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m             raise RuntimeError(\n\u001b[0;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\kewill\\anaconda\\envs\\AI\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 8000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import torch\n",
    "# torchvision包的主要功能是实现数据的处理，导入和预览等\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "start_time = time.time()\n",
    "# 对数据进行载入及有相应变换,将Compose看成一种容器，他能对多种数据变换进行组合\n",
    "# 传入的参数是一个列表，列表中的元素就是对载入的数据进行的各种变换操作\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5],std=[0.5])])\n",
    "# 首先获取手写数字的训练集和测试集\n",
    "# root 用于指定数据集在下载之后的存放路径\n",
    "# transform 用于指定导入数据集需要对数据进行那种变化操作\n",
    "# train是指定在数据集下载完成后需要载入那部分数据，\n",
    "# 如果设置为True 则说明载入的是该数据集的训练集部分\n",
    "# 如果设置为FALSE 则说明载入的是该数据集的测试集部分\n",
    "data_train = datasets.MNIST(root=\"C:/Users/wangkewang/Desktop/AI中阶/pytorch测试/data\",\n",
    "                           transform = transform,\n",
    "                            train = True,\n",
    "                            download = True)\n",
    " \n",
    "data_test = datasets.MNIST(root=\"C:/Users/wangkewang/Desktop/AI中阶/pytorch测试/data\",\n",
    "                           transform = transform,\n",
    "                            train = False)\n",
    "\n",
    "#数据预览和数据装载\n",
    "# 下面对数据进行装载，我们可以将数据的载入理解为对图片的处理，\n",
    "# 在处理完成后，我们就需要将这些图片打包好送给我们的模型进行训练 了  而装载就是这个打包的过程\n",
    "# dataset 参数用于指定我们载入的数据集名称\n",
    "# batch_size参数设置了每个包中的图片数据个数\n",
    "#  在装载的过程会将数据随机打乱顺序并进打包\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset =data_train,\n",
    "                                                batch_size = 64,\n",
    "                                                shuffle = True)\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset =data_test,\n",
    "                                                batch_size = 64,\n",
    "                                                shuffle = True)\n",
    "\n",
    "# 在装载完成后，我们可以选取其中一个批次的数据进行预览\n",
    "images,labels=next(iter(data_loader_train))\n",
    "img = torchvision.utils.make_grid(images)\n",
    " \n",
    "img = img.numpy().transpose(1,2,0)\n",
    "std = [0.5,0.5,0.5]\n",
    "mean = [0.5,0.5,0.5]\n",
    "img = img*std +mean\n",
    "#print(labels)\n",
    "print([labels[i] for i in range(64)])\n",
    "# 由于matplotlab中的展示图片无法显示，所以现在使用OpenCV中显示图片\n",
    "# plt.imshow(img)\n",
    "#cv2.imshow('win',img)\n",
    "#key_pressed=cv2.waitKey(0)\n",
    "#模型搭建和参数优化\n",
    "# 在顺利完成数据装载后，我们可以开始编写卷积神经网络模型的搭建和参数优化的代码\n",
    "#卷积层使用torch.nn.Conv2d类来搭建\n",
    "# 激活层使用torch.nn.ReLU 类方法来搭建\n",
    "# 池化层使用torch.nn.MaxPool2d类方法来搭建\n",
    "# 全连接层使用 torch.nn.Linear 类方法来搭建\n",
    " \n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(stride=2,kernel_size=2))\n",
    " \n",
    "        self.dense = torch.nn.Sequential(\n",
    "            torch.nn.Linear(14*14*128,1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p = 0.5),\n",
    "            torch.nn.Linear(1024,10)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(-1,14*14*128)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# 在编写完搭建卷积神经网络模型的代码后，我们可以对模型进行训练和参数进行优化了\n",
    "# 首先 定义在训练之前使用哪种损失函数和优化函数\n",
    "# 下面定义了计算损失值的损失函数使用的是交叉熵\n",
    "# 优化函数使用的额是Adam自适应优化算法\n",
    "model = Model()\n",
    "# 将所有的模型参数移动到GPU上\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "#print(model)\n",
    "\n",
    "# 卷积神经网络模型进行模型训练和参数优化的代码\n",
    "n_epochs = 5\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    print(\"Epoch  {}/{}\".format(epoch, n_epochs))\n",
    "    print(\"-\"*10)\n",
    "    for data in data_loader_train:\n",
    "        X_train , y_train = data\n",
    "        # 有GPU加下面这行，没有不用加\n",
    "        X_train, y_train = X_train.cuda(), y_train.cuda()\n",
    "        X_train , y_train = Variable(X_train),Variable(y_train)\n",
    "        # print(y_train)\n",
    "        outputs = model(X_train)\n",
    "        # print(outputs)\n",
    "        _,pred = torch.max(outputs.data,1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = cost(outputs,y_train)\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # running_loss += loss.data[0]\n",
    "        running_loss += loss.item()\n",
    "        running_correct += torch.sum(pred == y_train.data)\n",
    "        # print(\"ok\")\n",
    "        # print(\"**************%s\"%running_corrrect)\n",
    " \n",
    "    print(\"train ok \")\n",
    "    testing_correct = 0\n",
    "    for data in data_loader_test:\n",
    "        X_test,y_test = data\n",
    "        # 有GPU加下面这行，没有不用加\n",
    "        X_test, y_test = X_test.cuda(), y_test.cuda()\n",
    "        X_test,y_test = Variable(X_test),Variable(y_test)\n",
    "        outputs = model(X_test)\n",
    "        _, pred = torch.max(outputs,1)\n",
    "        testing_correct += torch.sum(pred == y_test.data)\n",
    "        # print(testing_correct)\n",
    " \n",
    "    print( \"Loss is :{:.4f},Train Accuracy is:{:.4f}%,Test Accuracy is:{:.4f}\".format(\n",
    "                 running_loss / len(data_train),100 * running_correct / len(data_train),\n",
    "                 100 * testing_correct / len(data_test)))\n",
    " \n",
    " \n",
    "stop_time = time.time()\n",
    "print(\"time is %s\" %(stop_time-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
